# scheduler/train_deep.py

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import os
from sqlalchemy.orm import Session
from collections import defaultdict

from scheduler.db import SessionLocal, ScheduleAssignment, Schedule, Nurse, SchedulePlan
from scheduler.ai_models import ShiftLSTM
from scheduler.generator import generate_initial_schedule
from scheduler.data_loader import load_plan_context
from scheduler.types import ShiftCode

# Config
MODEL_PATH = "lstm_model.pth"
MIN_SAMPLES_FOR_REAL_TRAIN = 500  # ì‹¤ì œ ë°ì´í„°ê°€ ì´ë³´ë‹¤ ì ìœ¼ë©´ ê°€ìƒ ë°ì´í„° ìƒì„±
SYNTHETIC_EPISODES = 50  # ê°€ìƒìœ¼ë¡œ ìƒì„±í•  ìŠ¤ì¼€ì¤„ ê°œìˆ˜
WINDOW_SIZE = 7  # ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´
BATCH_SIZE = 32
EPOCHS = 100
LEARNING_RATE = 0.01

SHIFT_TO_IDX = {'D': 0, 'E': 1, 'N': 2, 'O': 3}


def get_latest_plan_id(session):
    plan = session.query(SchedulePlan).order_by(SchedulePlan.id.desc()).first()
    return plan.id if plan else None


def prepare_sequences(assignments):
    """
    (nurse_id, date) ìˆœìœ¼ë¡œ ì •ë ¬ëœ ë°ì´í„°ë¥¼ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜
    """
    # 1. ê°„í˜¸ì‚¬ë³„ ì‹œê³„ì—´ ì •ë¦¬
    nurse_shifts = defaultdict(list)
    for r in assignments:
        s = r.shift_code
        if s == 'M':
            s = 'E'
        elif s == 'C':
            s = 'D'
        elif s == 'P':
            s = 'O'

        if s in SHIFT_TO_IDX:
            nurse_shifts[r.nurse_id].append(SHIFT_TO_IDX[s])

    # 2. Sliding Windowë¡œ ë°ì´í„°ì…‹ ìƒì„±
    X, y = [], []
    for nid, shifts in nurse_shifts.items():
        if len(shifts) <= WINDOW_SIZE: continue

        for i in range(len(shifts) - WINDOW_SIZE):
            seq_in = shifts[i: i + WINDOW_SIZE]
            target = shifts[i + WINDOW_SIZE]
            X.append(seq_in)
            y.append(target)

    return X, y


def generate_synthetic_data(session):
    """
    ë°ì´í„° ë¶€ì¡± ì‹œ Heuristic Generatorë¥¼ ì´ìš©í•´ ê°€ìƒ ë°ì´í„° ìƒì„±
    """
    print("âš ï¸ Not enough real data. Generating synthetic data for warm-up...")

    plan_id = get_latest_plan_id(session)
    if not plan_id:
        print("âŒ No Schedule Plan found. Cannot generate synthetic data.")
        return []

    try:
        ctx = load_plan_context(session, plan_id)
    except Exception as e:
        print(f"âŒ Failed to load context: {e}")
        return []

    synthetic_assignments = []

    for _ in range(SYNTHETIC_EPISODES):
        # Heuristic ëª¨ë“œë¡œ ìƒì„± (ê·œì¹™ ì¤€ìˆ˜ ìŠ¤ì¼€ì¤„)
        sch = generate_initial_schedule(ctx, seed=None, mode="heuristic")

        # DataFrameì„ ScheduleAssignment ê°ì²´ ë¦¬ìŠ¤íŠ¸ì²˜ëŸ¼ ë³€í™˜ (êµ¬ì¡°ë§Œ ë§ì¶¤)
        # ì‹¤ì œ DB ê°ì²´ê°€ ì•„ë‹ˆë¼ namedtupleì´ë‚˜ dict ìœ ì‚¬ ê°ì²´ë¡œ ë§Œë“¦
        class MockAssignment:
            def __init__(self, nid, sc):
                self.nurse_id = nid
                self.shift_code = sc

        # ë‚ ì§œ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•˜ê¸° ìœ„í•´ dates ìˆœíšŒ
        for d in ctx.dates:
            for n in ctx.nurses:
                shift = sch.get_shift(n.id, d)
                synthetic_assignments.append(MockAssignment(n.id, shift))

    print(f"âœ… Generated {len(synthetic_assignments)} synthetic shift records.")
    return synthetic_assignments


def train_model():
    session = SessionLocal()
    try:
        # 1. ì‹¤ì œ ë°ì´í„° ë¡œë“œ
        real_data = (
            session.query(ScheduleAssignment)
            .join(Schedule)
            .join(Nurse)
            .order_by(ScheduleAssignment.nurse_id, ScheduleAssignment.date)
            .all()
        )

        # 2. Cold Start ì²´í¬
        assignments = real_data
        if len(real_data) < MIN_SAMPLES_FOR_REAL_TRAIN:
            # ê°€ìƒ ë°ì´í„° ìƒì„± ë° í•©ì¹˜ê¸°
            syn_data = generate_synthetic_data(session)
            assignments = real_data + syn_data  # ë¦¬ìŠ¤íŠ¸ í•©ì¹˜ê¸°

        if not assignments:
            print("âŒ No data available for training.")
            return

        # 3. ë°ì´í„°ì…‹ êµ¬ì„±
        X_raw, y_raw = prepare_sequences(assignments)

        if not X_raw:
            print("âŒ Not enough sequences formed.")
            return

        X_tensor = torch.tensor(X_raw, dtype=torch.long)
        y_tensor = torch.tensor(y_raw, dtype=torch.long)

        # 4. ëª¨ë¸ ì´ˆê¸°í™”
        model = ShiftLSTM(num_shifts=4)
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

        # ê¸°ì¡´ ëª¨ë¸ì´ ìˆë‹¤ë©´ ë¡œë“œí•´ì„œ ì´ì–´ì„œ í•™ìŠµ (Fine-tuning)
        if os.path.exists(MODEL_PATH):
            try:
                model.load_state_dict(torch.load(MODEL_PATH))
                print("ğŸ”„ Loaded existing model for fine-tuning.")
            except:
                print("âš ï¸ Failed to load existing model. Starting fresh.")

        # 5. í•™ìŠµ ë£¨í”„
        model.train()
        print(f"ğŸš€ Starting training on {len(X_raw)} sequences...")

        dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)
        loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

        for epoch in range(EPOCHS):
            total_loss = 0
            for batch_X, batch_y in loader:
                optimizer.zero_grad()
                outputs = model(batch_X)  # (Batch, 4)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                total_loss += loss.item()

            if (epoch + 1) % 10 == 0:
                print(f"Epoch [{epoch + 1}/{EPOCHS}], Loss: {total_loss / len(loader):.4f}")

        # 6. ì €ì¥
        torch.save(model.state_dict(), MODEL_PATH)
        print(f"ğŸ’¾ Model saved to {MODEL_PATH}")

    except Exception as e:
        print(f"âŒ Training failed: {e}")
        import traceback
        traceback.print_exc()
    finally:
        session.close()


if __name__ == "__main__":
    train_model()